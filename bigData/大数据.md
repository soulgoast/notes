# 大数据

大数据的前世今生

​	Spark，是一种通用的大数据计算框架，正如传统大数据技术Hadoop的MapReduce、Hive引擎，以及Storm流式实时计算引擎等。

​	Spark包含了大数据领域常见的各种计算框架：

- Spark Core用于离线计算。
- Spark SQL用于交互式查询。
- Spark Streaming用于实时流式计算。
- Spark MLlib用于机器学习。
- Spark GraphX用于图计算。

​      Spark主要用于大数据计算，而Hadoop主要应用于大数据的存储（比如HDFS、Hive、HBase等），以及资源调度（Yarn）。

​      Spark + Hadoop的组合，是未来大数据领域最热门的组合，也是最有前景的组合！

​	Spark，是一种“One Stack ro rule them all”的大数据计算框架，期望使用一个技术栈就完美的解决大数据领域的各种计算任务。Apache官方对Spark的定义就是：通用的大数据快速处理引擎。

​	Spark使用Spark RDD、Spark SQL、Spark Streaming、MLlib、GrapgX成功解决了大数据领域中，离线批处理、交互式查询、实时流计算、机器学习与图计算等领域最重要的任务和问题。

​	Spark除了一站式的特点之外，另外一个重要的特点，就是基于内存进行计算，从而让它的速度可以达到MapReduce、Hive的数倍甚至数十倍。

​	现在已经有很多大公司正在生成环境下深度的使用Spark作为大数据的计算框架，包括eBay、Yahoo、BAT、网易、京东、华为、大众点评、优酷土豆、搜狗等。

​	Spark同时也获得了多个世界顶级IT厂商的支持，包括IBM、Intel等。 

![img](http://5b0988e595225.cdn.sohucs.com/images/20180620/1a89c06b1c454b0b89870fd724fd87f9.jpeg)

# Spark的特点

- 速度快：Spark基于内存进行计算（当然也有部分计算基于磁盘，比如shuffle）。
- 容易上手开发：Spark基于RDD的计算模型，比Hadoop的基于Map-Reduce的计算模型更加容易理解，更加容易上手开发，实现各种复杂功能，比如二次排序，topn等复杂操作时，更加快捷。
- 超强的通用性：Spark提供了Spark RDD、Spark SQL、Spark Streaming、Spark MLlib、Spark GraphX等技术组件，可以一站式地完成大数据领域的离线批处理、交互式查询、流式计算、机器学习、图计算等常见的任务。
- 集成Hadoop：Spark并不是要成为一个大数据领域的“独裁者”，一个人霸占大数据领域所有的“地盘”，而是与Hadoop进行了高度的集成，两者可以完美的配合使用。Hadoop的HDFS、Hive、HBase负责存储，Yarn负责资源的调度；Spark负责大数据计算。实际上，Spark+Hadoop的组合，是一种“double win”的组合。
- 极高的活跃度：社区活跃。

### Spark VS MapReduce

![1541431339019](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541431339019.png)

Spark SQL VS Hive

![1541431889906](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541431889906.png)

### Spark Streaming VS Storm

![1541432413435](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541432413435.png)

![1541433393543](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541433393543.png)



![1541433817770](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541433817770.png)

![1541434097971](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541434097971.png)

![1541434299651](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541434299651.png)

![1541434380524](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541434380524.png)

![1541434413139](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541434413139.png)

![1541434466904](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541434466904.png)

![1541434518283](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541434518283.png)

![1541434790888](C:\Users\m1896\AppData\Roaming\Typora\typora-user-images\1541434790888.png)



# 第三讲



# Linux

##### 目录结构介绍

```bash
[root@shizhan /]# ll
总用量 28
lrwxrwxrwx.   1 root root    7 7月  31 08:13 bin -> usr/bin
dr-xr-xr-x.   5 root root 4096 7月  31 08:30 boot
drwxr-xr-x.  20 root root 3260 11月  4 19:17 dev
drwxr-xr-x. 136 root root 8192 10月 27 11:49 etc
drwxr-xr-x.   5 root root   45 10月 27 08:24 home
lrwxrwxrwx.   1 root root    7 7月  31 08:13 lib -> usr/lib
lrwxrwxrwx.   1 root root    9 7月  31 08:13 lib64 -> usr/lib64
drwxr-xr-x.   2 root root    6 11月  5 2016 media
drwxr-xr-x.   3 root root   19 8月   1 21:25 mnt
drwxr-xr-x.   3 root root   16 7月  31 08:23 opt
dr-xr-xr-x. 150 root root    0 11月  4 19:17 proc
dr-xr-x---.  18 root root 4096 11月  4 19:20 root
drwxr-xr-x.  37 root root 1100 11月  4 19:17 run
lrwxrwxrwx.   1 root root    8 7月  31 08:13 sbin -> usr/sbin
drwxr-xr-x.   2 root root    6 11月  5 2016 srv
dr-xr-xr-x.  13 root root    0 11月  4 19:17 sys
drwxrwxrwt.  33 root root 4096 11月  4 19:18 tmp
drwxr-xr-x.  14 root root  228 10月 12 13:36 usr
drwxr-xr-x.  21 root root 4096 7月  31 15:25 var
```

目录介绍：

bin：



##### 局域网工作机制和网络地址配置

##### 文件操作

##### 文件权限操作

##### 常用系统登陆

##### SSH免密登陆配置

## 常见软件的安装

### jdk

### mysql

### tomcat

### yum



# hdfs

# mapreduce

# yarn

# storm

# Spark Core

### RDD

1. RDD是Spark提供的核心抽象，全称为Resilient Distributed Dataset，即弹性分布式数据集。
2. RDD在抽象上来说是一种元素集合，包含了数据。它是被分区的，分为多个分区，每个分区分布在集群中的不同节点，从而让RDD中的数据可以被并行操作。（分布式数据集）
3. RDD通常通过Hadoop上的文件，即HDFS文件或者Hive表，来进行创建；有时也可以通过应用程序中的集合来创建。
4. RDD最重要的特性就是提供了容错性，可以自动从节点失败中恢复过来。即如果某个节点上的RDD partition， 因为节点故障，导致数据丢失了，那么RDD会自动通过自己的数据来源重新计算该partition。这一切对使用者是透明的。
5. RDD的数据默认情况下存放在内存中，但是在内存资源不足时，Spark会自动将RDD数据写入磁盘。（弹性）

### Spark的核心编程时什么？

​	首先， 定义初始的RDD，也就是定义一个RDD是从哪里读取数据，hdfs、Linux本地文件、Hive、程序中的集合。然后，定义对RDD的计算操作，这个在Spark里称之为算子，map、reduce、flatMap、groupByKey，比mapreduce提供的map和reduce强大的太多太多了。第一个计算完了以后，数据可能就会到了新的一批节点上变成一个新的RDD。然后再次反复，针对新的RDD定义计算操作。最后，就是获得最终的数据，将数据保存起来。

### 开发wordcount程序

1. 用Java开发wordcount程序

   1. 配置maven环境
   2. 如何进行本地测试
   3. 如何使用Spark-submit提交到spark集群进行执行（spark-submit常用参数说明，spark-submit其实就类似于hadoop的hadoop jar命令）
2. 用Scala开发wordcount程序
   1. 下载scala ide for eclipse
   2. 
3. 用spark-shell开发wordcount程序




# SparkSQL